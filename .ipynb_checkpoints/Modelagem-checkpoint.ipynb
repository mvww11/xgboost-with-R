{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENEM 2019: Previsão das notas de Matemática a partir de dados socioeconômicos\n",
    "## Treinando um XGBoost\n",
    "\n",
    "\n",
    "Esse é o mesmo projeto que já realizei em python. A novidade aqui é que usaremos a linguagem R para treinar um modelo XGBoost. Por isso, nesse notebook, não faremos a modelagem completa (e.g. conjunto de validação e teste, refinamento de hiperparâmetros, etc). A modelagem completa pode ser vista [aqui](https://www.kaggle.com/hurzzz/enem-2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered S3 methods overwritten by 'ggplot2':\n",
      "  method         from \n",
      "  [.quosures     rlang\n",
      "  c.quosures     rlang\n",
      "  print.quosures rlang\n",
      "Registered S3 method overwritten by 'rvest':\n",
      "  method            from\n",
      "  read_xml.response xml2\n",
      "-- Attaching packages --------------------------------------- tidyverse 1.2.1 --\n",
      "v ggplot2 3.1.1       v purrr   0.3.2  \n",
      "v tibble  2.1.1       v dplyr   0.8.0.1\n",
      "v tidyr   0.8.3       v stringr 1.4.0  \n",
      "v readr   1.3.1       v forcats 0.4.0  \n",
      "-- Conflicts ------------------------------------------ tidyverse_conflicts() --\n",
      "x dplyr::filter() masks stats::filter()\n",
      "x dplyr::lag()    masks stats::lag()\n",
      "Loading required package: lattice\n",
      "\n",
      "Attaching package: 'caret'\n",
      "\n",
      "The following object is masked from 'package:purrr':\n",
      "\n",
      "    lift\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t3633513 obs. of  6 variables:\n",
      " $ Q005      : int  7 5 3 5 4 4 4 4 3 3 ...\n",
      " $ Q006      : int  1 1 2 2 2 1 1 1 1 2 ...\n",
      " $ TP_SEXO   : int  0 1 1 0 1 1 0 1 0 1 ...\n",
      " $ max_escol : int  4 4 4 2 1 1 3 4 4 3 ...\n",
      " $ perCapita : int  1 1 3 2 2 1 1 1 1 3 ...\n",
      " $ NU_NOTA_MT: num  369 416 572 605 582 ...\n"
     ]
    }
   ],
   "source": [
    "#carregando as bibliotecas\n",
    "library(tidyverse)\n",
    "library(caret) #para o train/test split\n",
    "library(xgboost)\n",
    "\n",
    "#carregando os dados\n",
    "X <- read.csv('X_final.csv')\n",
    "y  <- read.csv('y_final.csv')\n",
    "\n",
    "full_data <- cbind(X, y)\n",
    "\n",
    "str(full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividindo os dados em conjunto de treinamento e validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train/test split\n",
    "trainIndex <- createDataPartition(full_data$NU_NOTA_MT, p = .9, \n",
    "                                  list = FALSE, \n",
    "                                  times = 1)\n",
    "\n",
    "full_data_train <- full_data[ trainIndex,]\n",
    "full_data_test  <- full_data[-trainIndex,]\n",
    "\n",
    "X_train <- X[ trainIndex,]\n",
    "y_train <- y[ trainIndex,]\n",
    "\n",
    "X_test  <- X[-trainIndex,]\n",
    "y_test  <- y[-trainIndex,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgbcv <- xgb.cv( params = {}, data = X_train, nrounds = 500, nfold = 5, showsd = T, stratified = T, print_every_n = 40, early_stopping_rounds = 10, maximize = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parâmetros ótimos encontrados por bayesian search no projeto feito em python\n",
    "params <- list(booster = \"gbtree\"\n",
    "              , objective = \"reg:squarederror\"\n",
    "              , colsample_bytree = 0.7971305\n",
    "              , eta = 0.122620\n",
    "              , reg_alpha = 0.282408\n",
    "              , reg_lambda = 0.84137\n",
    "              , eval_metric = 'rmse'\n",
    "              , min_child_weight = 275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colocando nossos dados no formato aceito pela biblioteca xgboost\n",
    "xgb_train = xgb.DMatrix(data = as.matrix(X_train), label = as.matrix(y_train))\n",
    "xgb_test = xgb.DMatrix(data = as.matrix(X_test), label = as.matrix(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinando o modelo\n",
    "model <- xgb.train(params = params\n",
    "                 , data = xgb_train\n",
    "                 , nrounds = 173\n",
    "                 , verbose = 1\n",
    "                 , print_every_n = 2\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  8686.489 MAE:  75.23165  RMSE:  93.20133"
     ]
    }
   ],
   "source": [
    "#fazendo a previsão no conjunto de validação\n",
    "pred_y = predict(model, xgb_test)\n",
    "\n",
    "#avaliando o modelo\n",
    "mse = mean((y_test - pred_y)^2)\n",
    "mae = caret::MAE(y_test, pred_y)\n",
    "rmse = caret::RMSE(y_test, pred_y)\n",
    "\n",
    "cat(\"MSE: \", mse, \"MAE: \", mae, \" RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O RMSE no conjunto de teste (exemplo nunca vistos antes pelo modelo) foi de aproximadamente 93, compatível com o valor encontrado no projeto desenvolvido em python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
